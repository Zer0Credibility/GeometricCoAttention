{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e7211b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import optim\n",
    "import wandb\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "\n",
    "import torch_geometric as tg\n",
    "import torchmetrics\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "\n",
    "from GraphCoAttention.datasets.HeterogenousDDI import HeteroDrugDrugInteractionData\n",
    "from GraphCoAttention.nn.models.HeterogenousCoAttention import HeteroGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a8a67fc-dd89-4da1-ae5c-59f23afbd2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(pl.LightningModule):\n",
    "    def __init__(self, root_dir, hidden_dim=25, n_cycles=16, n_head=1, dropout=0.1, lr=0.001, bs=2):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "        self.dataset = HeteroDrugDrugInteractionData(root=self.root_dir)\n",
    "        self.dataset = self.dataset.shuffle()\n",
    "\n",
    "        self.dataset = self.dataset[:10]\n",
    "\n",
    "        self.num_node_types = len(self.dataset[0].x_dict)\n",
    "        self.num_workers = 32\n",
    "        self.n_cycles = n_cycles\n",
    "        self.n_head = n_head\n",
    "        self.dropout = dropout\n",
    "        self.batch_size = bs\n",
    "        self.lr = lr\n",
    "\n",
    "        # self.num_features = self.dataset.num_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        wandb.config.hidden_dim = self.hidden_dim\n",
    "        wandb.config.n_layers = self.n_cycles\n",
    "        wandb.config.n_head = self.n_head\n",
    "        wandb.config.dropout = self.dropout\n",
    "\n",
    "        self.HeterogenousCoAttention = HeteroGNN(hidden_channels=self.hidden_dim, out_channels=1, num_layers=self.n_cycles,\n",
    "                                                 batch_size=self.batch_size, num_node_types=self.num_node_types,\n",
    "                                                 num_heads=self.n_head)\n",
    "\n",
    "        # self.CoAttention = CoAttention(hidden_channels=self.hidden_dim, encoder=self.encoder,\n",
    "        #                                outer=self.outer, inner=self.inner,\n",
    "        #                                update=self.update, readout=self.readout,\n",
    "        #                                n_cycles=self.n_cycles, batch_size=self.batch_size, n_head=self.n_head)\n",
    "\n",
    "        self.bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, batch, *args, **kwargs):\n",
    "\n",
    "        logits = self.HeterogenousCoAttention(batch.x_dict, batch.edge_index_dict, batch)\n",
    "\n",
    "        # print(logits)\n",
    "        # exit()\n",
    "        # logits = self.CoAttention(data)\n",
    "        # logits = torch.sigmoid(torch.mean(logits))\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, data, batch_idx):\n",
    "        logits = self(data)\n",
    "        y_pred = logits.squeeze()\n",
    "        y_true = data.binary_y.float()\n",
    "\n",
    "        bce = self.bce_loss(input=y_pred, target=y_true)\n",
    "        # self.log('train_loss', bce)\n",
    "        wandb.log({\"train/loss\": bce})\n",
    "        wandb.log({'train/y_pred': y_pred})\n",
    "        wandb.log({'train/y_true': y_true})\n",
    "        return {'loss': bce}  # , 'train_accuracy': acc, 'train_f1': f1}\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "\n",
    "        # print(val_batch.binary_y.float())\n",
    "\n",
    "        logits = self(val_batch)\n",
    "        y_pred = logits.squeeze()\n",
    "        y_true = val_batch.binary_y.float()\n",
    "\n",
    "        bce_loss = self.bce_loss(input=y_pred, target=y_true)\n",
    "        # self.log('validation_loss', bce_loss)\n",
    "        # self.log('Predicted', y_pred)\n",
    "        # self.log('Actual', y_true)\n",
    "        wandb.log({\"val/loss\": bce_loss})\n",
    "        return {'loss': bce_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.lr, betas=(0.28, 0.93), weight_decay=0.01)\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, '25,35', gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return tg.loader.DataLoader(list(self.dataset), batch_size=self.batch_size,\n",
    "                                    num_workers=self.num_workers, pin_memory=False, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return tg.loader.DataLoader(list(self.dataset), batch_size=self.batch_size,\n",
    "                                    num_workers=self.num_workers, pin_memory=False, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa3a0924-bf5a-4c40-a425-a43ff898bc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [....................................................] 35688667 / 35688667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ./._bio-decagon-combo.csv      STITCH 2 Polypharmacy Side Effect  \\\n",
      "0                    CID000002173  CID000003345                 C0151714   \n",
      "1                    CID000002173  CID000003345                 C0035344   \n",
      "2                    CID000002173  CID000003345                 C0004144   \n",
      "3                    CID000002173  CID000003345                 C0002063   \n",
      "4                    CID000002173  CID000003345                 C0004604   \n",
      "...                           ...           ...                      ...   \n",
      "4649437              CID000003461  CID000003954                 C0035410   \n",
      "4649438              CID000003461  CID000003954                 C0043096   \n",
      "4649439              CID000003461  CID000003954                 C0003962   \n",
      "4649440              CID000003461  CID000003954                 C0038999   \n",
      "4649441                       NaN           NaN                      NaN   \n",
      "\n",
      "                   Side Effect Name  \n",
      "0                   hypermagnesemia  \n",
      "1        retinopathy of prematurity  \n",
      "2                       atelectasis  \n",
      "3                         alkalosis  \n",
      "4                         Back Ache  \n",
      "...                             ...  \n",
      "4649437              rhabdomyolysis  \n",
      "4649438              loss of weight  \n",
      "4649439                     ascites  \n",
      "4649440                     bulging  \n",
      "4649441                         NaN  \n",
      "\n",
      "[4649442 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4649442/4649442 [00:26<00:00, 176720.40it/s]\n",
      "100%|███████████████████████████████████████| 1000/1000 [14:36<00:00,  1.14it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 2997.92it/s]\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 2790.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msyntensor\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/syntensor/flux/runs/671jnhzt\" target=\"_blank\">lunar-shadow-354</a></strong> to <a href=\"https://wandb.ai/syntensor/flux\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                    | Type              | Params\n",
      "--------------------------------------------------------------\n",
      "0 | HeterogenousCoAttention | HeteroGNN         | 19.2 K\n",
      "1 | bce_loss                | BCEWithLogitsLoss | 0     \n",
      "--------------------------------------------------------------\n",
      "19.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.2 K    Total params\n",
      "0.077     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 499:  50%|██████      | 5/10 [00:03<00:03,  1.29it/s, loss=0.611, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 499:  70%|████████▍   | 7/10 [00:06<00:02,  1.02it/s, loss=0.611, v_num=0]\u001b[A\n",
      "Validating:  40%|█████████████▏                   | 2/5 [00:04<00:06,  2.03s/it]\u001b[A\n",
      "Epoch 499:  90%|██████████▊ | 9/10 [00:09<00:01,  1.07s/it, loss=0.611, v_num=0]\u001b[A\n",
      "Validating:  80%|██████████████████████████▍      | 4/5 [00:07<00:01,  1.57s/it]\u001b[A\n",
      "Epoch 499: 100%|███████████| 10/10 [00:11<00:00,  1.18s/it, loss=0.611, v_num=0]\u001b[A\n",
      "Epoch 999:  60%|███████▏    | 6/10 [00:03<00:02,  1.54it/s, loss=0.611, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  20%|██████▌                          | 1/5 [00:01<00:06,  1.71s/it]\u001b[A\n",
      "Epoch 999:  80%|█████████▌  | 8/10 [00:05<00:01,  1.39it/s, loss=0.611, v_num=0]\u001b[A\n",
      "Validating:  60%|███████████████████▊             | 3/5 [00:02<00:00,  2.00it/s]\u001b[A\n",
      "Epoch 999: 100%|███████████| 10/10 [00:06<00:00,  1.66it/s, loss=0.611, v_num=0]\u001b[A\n",
      "Epoch 999: 100%|███████████| 10/10 [00:06<00:00,  1.54it/s, loss=0.611, v_num=0]\u001b[A\n",
      "Epoch 1499:  60%|██████▌    | 6/10 [00:03<00:02,  1.59it/s, loss=0.611, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  20%|██████▌                          | 1/5 [00:01<00:06,  1.72s/it]\u001b[A\n",
      "Epoch 1499:  80%|████████▊  | 8/10 [00:05<00:01,  1.41it/s, loss=0.611, v_num=0]\u001b[A\n",
      "Validating:  60%|███████████████████▊             | 3/5 [00:02<00:01,  1.98it/s]\u001b[A\n",
      "Epoch 1499: 100%|██████████| 10/10 [00:05<00:00,  1.67it/s, loss=0.611, v_num=0]\u001b[A\n",
      "Epoch 1499: 100%|██████████| 10/10 [00:06<00:00,  1.56it/s, loss=0.611, v_num=0]\u001b[A\n",
      "Epoch 1999:  60%|██████▌    | 6/10 [00:03<00:02,  1.60it/s, loss=0.611, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  20%|██████▌                          | 1/5 [00:01<00:06,  1.70s/it]\u001b[A\n",
      "Epoch 1999:  80%|████████▊  | 8/10 [00:05<00:01,  1.43it/s, loss=0.611, v_num=0]\u001b[A\n",
      "Validating:  60%|███████████████████▊             | 3/5 [00:02<00:01,  1.98it/s]\u001b[A\n",
      "Epoch 1999: 100%|██████████| 10/10 [00:05<00:00,  1.69it/s, loss=0.611, v_num=0]\u001b[A\n",
      "Epoch 1999: 100%|██████████| 10/10 [00:06<00:00,  1.57it/s, loss=0.611, v_num=0]\u001b[A\n",
      "Epoch 1999: 100%|██████████| 10/10 [00:06<00:00,  1.52it/s, loss=0.611, v_num=0]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "data_dir = os.path.join('GraphCoAttention', 'data')\n",
    "wandb_logger = WandbLogger(project='flux', log_model='all')\n",
    "trainer = pl.Trainer(gpus=[0], max_epochs=2000, check_val_every_n_epoch=500, accumulate_grad_batches=1)\n",
    "trainer.fit(Learner(data_dir, bs=2, lr=0.0005, n_cycles=30, hidden_dim=10, n_head=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3eac1f-e3b4-480e-a276-64d56cf70698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
