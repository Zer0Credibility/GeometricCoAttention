{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "180d7b6c-11b1-4efd-bae7-48749f1bb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import optim\n",
    "import wandb\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "\n",
    "import torch_geometric as tg\n",
    "import torchmetrics\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Parameter, Sequential, ReLU, GRU\n",
    "from torch.nn import functional as F\n",
    "import torch_geometric as tg\n",
    "\n",
    "from GraphCoAttention.datasets.HeterogenousDDI import HeteroDrugDrugInteractionData, HeteroQM9\n",
    "\n",
    "\n",
    "from torch_geometric.nn import GATConv, HeteroConv, Linear, GATv2Conv, NNConv, Set2Set\n",
    "from torch_geometric.nn.glob import global_mean_pool, global_add_pool\n",
    "from torch.nn import LeakyReLU\n",
    "\n",
    "# from GraphCoAttention.data.MultipartiteData import BipartitePairData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "763635f1-211b-4626-949c-9a682d12a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, outer_out_channels, inner_out_channels,\n",
    "                 num_layers, batch_size, num_node_types, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.heads = num_heads\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                ('x_i', 'inner_edge_i', 'x_i'): GATv2Conv(-1, self.hidden_channels, heads=num_heads),\n",
    "                ('x_j', 'inner_edge_j', 'x_j'): GATv2Conv(-1, self.hidden_channels, heads=num_heads),\n",
    "                # ('x_i', 'outer_edge_ij', 'x_j'): GATv2Conv(-1, self.hidden_channels, heads=num_heads),\n",
    "                # ('x_j', 'outer_edge_ji', 'x_i'): GATv2Conv(-1, self.hidden_channels, heads=num_heads),\n",
    "                # ('x_i', 'inner_edge_i', 'x_i'): GATv2Conv(-1, self.hidden_channels, heads=num_heads),\n",
    "                # ('x_j', 'inner_edge_j', 'x_j'): GATv2Conv(-1, self.hidden_channels, heads=num_heads),\n",
    "            }, aggr='sum')\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.lin = Linear(self.hidden_channels, outer_out_channels)\n",
    "\n",
    "        self.lin_i = Linear(self.hidden_channels, inner_out_channels)\n",
    "        self.lin_j = Linear(self.hidden_channels, inner_out_channels)\n",
    "        # self.hlin = tg.nn.HeteroLinear(hidden_channels, out_channels, num_node_types=num_node_types)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, d):\n",
    "\n",
    "        # x_dict, edge_index_dict = x_dict, edge_index_dict\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: torch.tanh(torch.sum(x.view(-1, self.heads, self.hidden_channels), dim=1))\n",
    "                      for key, x in x_dict.items()}\n",
    "\n",
    "            # [print(key, x.shape) for key, x in x_dict.items()]\n",
    "            # [print(key, x.view(-1, self.heads, self.hidden_channels).shape) for key, x in x_dict.items()]\n",
    "            # [print(key, torch.mean(x.view(-1, self.heads, self.hidden_channels), dim=1).shape) for key, x in x_dict.items()]\n",
    "\n",
    "        # p_i = F.leaky_relu(global_add_pool(x_dict['x_i'], batch=d['x_i'].batch, size=self.batch_size).unsqueeze(1))\n",
    "        # p_j = F.leaky_relu(global_add_pool(x_dict['x_j'], batch=d['x_j'].batch, size=self.batch_size).unsqueeze(1))\n",
    "\n",
    "        # p_i = global_add_pool(x_dict['x_i'], batch=d['x_i'].batch, size=self.batch_size).unsqueeze(1).sigmoid()\n",
    "        # p_j = global_add_pool(x_dict['x_j'], batch=d['x_j'].batch, size=self.batch_size).unsqueeze(1).sigmoid()\n",
    "\n",
    "        p_i = global_add_pool(x_dict['x_i'], batch=d['x_i'].batch, size=self.batch_size).unsqueeze(1).tanh()\n",
    "        p_j = global_add_pool(x_dict['x_j'], batch=d['x_j'].batch, size=self.batch_size).unsqueeze(1).tanh()\n",
    "\n",
    "        y_i_ = self.lin_i(p_i)\n",
    "        y_j_ = self.lin_j(p_j)\n",
    "\n",
    "        x = torch.cat([p_i, p_j], dim=1)\n",
    "        x = torch.sum(x, dim=1)\n",
    "\n",
    "        logits = self.lin(x).sigmoid()\n",
    "        return logits, y_i_, y_j_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f5a611c-2fed-4d57-b000-da3c9bb41a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2tg0cuoh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23456... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sleek-valley-25</strong>: <a href=\"https://wandb.ai/katharina_z/GeometricCoAttention-Notebooks/runs/2tg0cuoh\" target=\"_blank\">https://wandb.ai/katharina_z/GeometricCoAttention-Notebooks/runs/2tg0cuoh</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220108_131215-2tg0cuoh/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2tg0cuoh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/katharina_z/GeometricCoAttention-Notebooks/runs/36qkamjb\" target=\"_blank\">valiant-darkness-26</a></strong> to <a href=\"https://wandb.ai/katharina_z/GeometricCoAttention-Notebooks\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Learner' object has no attribute 'num_node_types'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22297/3981260291.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mwandb_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWandbLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'flux'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_val_every_n_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulate_grad_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_22297/3981260291.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, lr)\u001b[0m\n\u001b[1;32m     21\u001b[0m         self.HeterogenousCoAttention = HeteroGNN(hidden_channels=self.hidden_dim, outer_out_channels=1,\n\u001b[1;32m     22\u001b[0m                                          \u001b[0minner_out_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_cycles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_node_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_node_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                                          num_heads=self.n_head)\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1131\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Learner' object has no attribute 'num_node_types'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread ChkStopThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/envs/st/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "Exception in thread NetStatThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ray/anaconda3/envs/st/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "    self.run()\n",
      "  File \"/home/ray/anaconda3/envs/st/lib/python3.8/threading.py\", line 870, in run\n",
      "  File \"/home/ray/anaconda3/envs/st/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ray/anaconda3/envs/st/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 152, in check_network_status\n",
      "    status_response = self._interface.communicate_network_status()\n",
      "  File \"/home/ray/anaconda3/envs/st/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\", line 125, in communicate_network_status\n",
      "    resp = self._communicate_network_status(status)\n",
      "  File \"/home/ray/anaconda3/envs/st/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 388, in _communicate_network_status\n",
      "    resp = self._communicate(req, local=True)\n",
      "  File \"/home/ray/anaconda3/envs/st/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 213, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"/home/ray/anaconda3/envs/st/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 218, in _communicate_async\n",
      "        raise Exception(\"The wandb backend process has shutdown\")\n",
      "Exception: The wandb backend process has shutdown\n",
      "self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ray/anaconda3/envs/st/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 170, in check_status\n",
      "    status_response = self._interface.communicate_stop_status()\n",
      "  File \"/home/ray/anaconda3/envs/st/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\", line 114, in communicate_stop_status\n",
      "    resp = self._communicate_stop_status(status)\n",
      "  File \"/home/ray/anaconda3/envs/st/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 378, in _communicate_stop_status\n",
      "    resp = self._communicate(req, local=True)\n",
      "  File \"/home/ray/anaconda3/envs/st/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 213, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"/home/ray/anaconda3/envs/st/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 218, in _communicate_async\n",
      "    raise Exception(\"The wandb backend process has shutdown\")\n",
      "Exception: The wandb backend process has shutdown\n"
     ]
    }
   ],
   "source": [
    "class Learner(pl.LightningModule):\n",
    "    def __init__(self, root_dir, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "        # self.dataset = HeteroDrugDrugInteractionData(root=self.root_dir)\n",
    "        self.dataset = HeteroQM9(root=self.root_dir)\n",
    "        self.dataset = self.dataset.shuffle()\n",
    "        \n",
    "        self.dataset = self.dataset[:10]\n",
    "\n",
    "        self.num_workers = 32\n",
    "        self.lr = lr\n",
    "        # self.num_node_types = len(self.dataset[0].x_dict)\n",
    "        self.n_cycles = 16\n",
    "        self.dropout = 0.1\n",
    "        self.batch_size = 2\n",
    "        self.lr = 0.001\n",
    "        self.hidden_dim = 25\n",
    "        \n",
    "        self.HeterogenousCoAttention = HeteroGNN(hidden_channels=self.hidden_dim, outer_out_channels=1,\n",
    "                                         inner_out_channels=15, num_layers=self.n_cycles,\n",
    "                                         batch_size=self.batch_size, num_node_types=self.num_node_types,\n",
    "                                         num_heads=self.n_head)\n",
    "\n",
    "        self.bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "        self.mse_loss = torch.nn.MSELoss()\n",
    "        \n",
    "        \n",
    "    def forward(self, batch, *args, **kwargs):\n",
    "\n",
    "        y_ij, y_i_, y_j_ = self.Net(batch.x_dict, batch.edge_index_dict, batch)\n",
    "\n",
    "        # logits = self.CoAttention(data)\n",
    "        # logits = torch.sigmoid(torch.mean(logits))\n",
    "        return y_ij, y_i_, y_j_\n",
    "\n",
    "    def training_step(self, data, batch_idx):\n",
    "        y_ij, y_i_, y_j_ = self(data)\n",
    "        y_pred = y_ij.squeeze()\n",
    "        y_true = data.binary_y.float()\n",
    "\n",
    "        mse1 = self.mse_loss(input=y_i_.flatten(), target=data['y_i'].y)\n",
    "        mse2 = self.mse_loss(input=y_j_.flatten(), target=data['y_j'].y)\n",
    "        mse = mse1 + mse2\n",
    "        bce = self.bce_loss(input=y_pred, target=y_true)\n",
    "        loss = mse\n",
    "\n",
    "        # self.log('train_loss', bce)\n",
    "        wandb.log({\"train/loss\": loss})\n",
    "        wandb.log({'train/y_pred': y_pred})\n",
    "        wandb.log({'train/y_true': y_true})\n",
    "        return {'loss': loss}  # , 'train_accuracy': acc, 'train_f1': f1}\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "\n",
    "        # print(val_batch.binary_y.float())\n",
    "\n",
    "        y_ij, y_i_, y_j_ = self(val_batch)\n",
    "        y_pred = y_ij.squeeze()\n",
    "        y_true = val_batch.binary_y.float()\n",
    "\n",
    "        mse1 = self.mse_loss(input=y_i_.flatten(), target=val_batch['y_i'].y)\n",
    "        mse2 = self.mse_loss(input=y_j_.flatten(), target=val_batch['y_j'].y)\n",
    "        mse = mse1 + mse2\n",
    "        bce = self.bce_loss(input=y_pred, target=y_true)\n",
    "        loss = bce + mse\n",
    "        # self.log('validation_loss', bce_loss)\n",
    "        # self.log('Predicted', y_pred)\n",
    "        # self.log('Actual', y_true)\n",
    "        wandb.log({\"val/loss\": loss})\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.lr, betas=(0.28, 0.93), weight_decay=0.01)\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, '25,35', gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return tg.loader.DataLoader(list(self.dataset),\n",
    "                                    num_workers=self.num_workers, pin_memory=False, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return tg.loader.DataLoader(list(self.dataset), \n",
    "                                    num_workers=self.num_workers, pin_memory=False, shuffle=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "    data_dir = os.path.join('GraphCoAttention', 'data')\n",
    "    wandb.init()\n",
    "    wandb_logger = WandbLogger(project='flux', log_model='all')\n",
    "    trainer = pl.Trainer(gpus=[0], max_epochs=2000, check_val_every_n_epoch=500, accumulate_grad_batches=1)\n",
    "    trainer.fit(Learner(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f30965-412d-4777-88a5-93879a4e1d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
