{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8b38c88-c70c-4e2c-aaaf-3c282655eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import optim\n",
    "import wandb\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "\n",
    "import torch_geometric as tg\n",
    "import torchmetrics\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "\n",
    "from GraphCoAttention.datasets.HeterogenousDDI import HeteroDrugDrugInteractionData, HeteroQM9\n",
    "# from GraphCoAttention.nn.models.CoAttention import CoAttention\n",
    "from GraphCoAttention.nn.models.HeterogenousCoAttention import HeteroGNN\n",
    "# from GraphCoAttention.nn.conv.GATConv import GATConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7205f1a6-f357-4e41-b458-1174265be84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import functional as F\n",
    "import torch_geometric as tg\n",
    "\n",
    "from torch_geometric.nn import GATConv, HeteroConv, Linear, GATv2Conv\n",
    "from torch_geometric.nn.glob import global_mean_pool, global_add_pool\n",
    "from torch.nn import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d5f9ab6-463e-4f4e-b6d4-52d1061f0486",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, outer_out_channels, inner_out_channels,\n",
    "                 num_layers, batch_size, num_node_types, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.heads = num_heads\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                ('x_i', 'inner_edge_i', 'x_i'): GATv2Conv(-1, self.hidden_channels, heads=num_heads),\n",
    "                ('x_j', 'inner_edge_j', 'x_j'): GATv2Conv(-1, self.hidden_channels, heads=num_heads),\n",
    "                ('x_i', 'outer_edge_ij', 'x_j'): GATv2Conv(-1, self.hidden_channels, heads=num_heads),\n",
    "                ('x_j', 'outer_edge_ji', 'x_i'): GATv2Conv(-1, self.hidden_channels, heads=num_heads),\n",
    "                ('x_i', 'inner_edge_i', 'x_i'): GATv2Conv(-1, self.hidden_channels, heads=num_heads),\n",
    "                ('x_j', 'inner_edge_j', 'x_j'): GATv2Conv(-1, self.hidden_channels, heads=num_heads),\n",
    "            }, aggr='sum')\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.lin = Linear(self.hidden_channels, outer_out_channels)\n",
    "\n",
    "        self.lin_i = Linear(self.hidden_channels, inner_out_channels)\n",
    "        self.lin_j = Linear(self.hidden_channels, inner_out_channels)\n",
    "        # self.hlin = tg.nn.HeteroLinear(hidden_channels, out_channels, num_node_types=num_node_types)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, d):\n",
    "\n",
    "        # x_dict, edge_index_dict = x_dict, edge_index_dict\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: torch.tanh(torch.sum(x.view(-1, self.heads, self.hidden_channels), dim=1))\n",
    "                      for key, x in x_dict.items()}\n",
    "\n",
    "            # [print(key, x.shape) for key, x in x_dict.items()]\n",
    "            # [print(key, x.view(-1, self.heads, self.hidden_channels).shape) for key, x in x_dict.items()]\n",
    "            # [print(key, torch.mean(x.view(-1, self.heads, self.hidden_channels), dim=1).shape) for key, x in x_dict.items()]\n",
    "\n",
    "        # p_i = F.leaky_relu(global_add_pool(x_dict['x_i'], batch=d['x_i'].batch, size=self.batch_size).unsqueeze(1))\n",
    "        # p_j = F.leaky_relu(global_add_pool(x_dict['x_j'], batch=d['x_j'].batch, size=self.batch_size).unsqueeze(1))\n",
    "\n",
    "        # p_i = global_add_pool(x_dict['x_i'], batch=d['x_i'].batch, size=self.batch_size).unsqueeze(1).sigmoid()\n",
    "        # p_j = global_add_pool(x_dict['x_j'], batch=d['x_j'].batch, size=self.batch_size).unsqueeze(1).sigmoid()\n",
    "\n",
    "        p_i = global_add_pool(x_dict['x_i'], batch=d['x_i'].batch, size=self.batch_size).unsqueeze(1).tanh()\n",
    "        p_j = global_add_pool(x_dict['x_j'], batch=d['x_j'].batch, size=self.batch_size).unsqueeze(1).tanh()\n",
    "\n",
    "        y_i_ = self.lin_i(p_i)\n",
    "        y_j_ = self.lin_j(p_j)\n",
    "\n",
    "        x = torch.cat([p_i, p_j], dim=1)\n",
    "        x = torch.sum(x, dim=1)\n",
    "\n",
    "        logits = self.lin(x).sigmoid()\n",
    "        return logits, y_i_, y_j_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c372d986-1fd7-45f1-9bbd-f726b75bee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(pl.LightningModule):\n",
    "    def __init__(self, root_dir, hidden_dim=25, n_cycles=16, n_head=1, dropout=0.1, lr=0.001, bs=2):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "        self.ddi_dataset = HeteroDrugDrugInteractionData(root=self.root_dir).shuffle()[:100]  # .shuffle()\n",
    "        self.qm9_dataset = HeteroQM9(root=self.root_dir).shuffle()[:100]  # .shuffle()\n",
    "\n",
    "        # self.dataset = self.dataset[:10]\n",
    "\n",
    "        self.num_node_types = len(self.qm9_dataset[0].x_dict)\n",
    "        self.num_workers = 32\n",
    "        self.n_cycles = n_cycles\n",
    "        self.n_head = n_head\n",
    "        self.dropout = dropout\n",
    "        self.batch_size = bs\n",
    "        self.lr = lr\n",
    "\n",
    "        # self.num_features = self.dataset.num_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        wandb.config.hidden_dim = self.hidden_dim\n",
    "        wandb.config.n_layers = self.n_cycles\n",
    "        wandb.config.n_head = self.n_head\n",
    "        wandb.config.dropout = self.dropout\n",
    "\n",
    "        # self.encoder = GATConv(self.num_features, self.hidden_dim, heads=self.n_head, dropout=self.dropout)\n",
    "        #\n",
    "        # self.inner = GATConv(self.hidden_dim * self.n_head, self.hidden_dim, heads=self.n_head,\n",
    "        #                      add_self_loops=True, bipartite=False, dropout=self.dropout)\n",
    "        # self.outer = GATConv(self.hidden_dim * self.n_head, self.hidden_dim, heads=self.n_head, add_self_loops=True,\n",
    "        #                      concat=False, bipartite=True, dropout=self.dropout)\n",
    "        #\n",
    "        # self.update = tg.nn.dense.Linear(self.hidden_dim * self.n_head + self.hidden_dim, self.hidden_dim * self.n_head)\n",
    "        # # self.update = GATConv(self.hidden_dim*self.n_head+self.hidden_dim, self.hidden_dim, heads=self.n_head,\n",
    "        # #                       add_self_loops=True, bipartite=False, dropout=self.dropout)\n",
    "        #\n",
    "        # self.readout = tg.nn.dense.Linear(in_channels=2 * self.hidden_dim, out_channels=1)\n",
    "        # # self.readout = GATConv(self.hidden_dim*self.n_head, self.hidden_dim, heads=1,\n",
    "        # #                        add_self_loops=True, bipartite=False, dropout=self.dropout)\n",
    "\n",
    "        self.HeterogenousCoAttention = HeteroGNN(hidden_channels=self.hidden_dim, outer_out_channels=1,\n",
    "                                                 inner_out_channels=15, num_layers=self.n_cycles,\n",
    "                                                 batch_size=self.batch_size, num_node_types=self.num_node_types,\n",
    "                                                 num_heads=self.n_head)\n",
    "\n",
    "        # self.CoAttention = CoAttention(hidden_channels=self.hidden_dim, encoder=self.encoder,\n",
    "        #                                outer=self.outer, inner=self.inner,\n",
    "        #                                update=self.update, readout=self.readout,\n",
    "        #                                n_cycles=self.n_cycles, batch_size=self.batch_size, n_head=self.n_head)\n",
    "\n",
    "        self.bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "        self.mse_loss = torch.nn.L1Loss()\n",
    "\n",
    "    def forward(self, batch, *args, **kwargs):\n",
    "\n",
    "        y_ij, y_i_, y_j_ = self.HeterogenousCoAttention(batch.x_dict, batch.edge_index_dict, batch)\n",
    "\n",
    "        # logits = self.CoAttention(data)\n",
    "        # logits = torch.sigmoid(torch.mean(logits))\n",
    "        return y_ij, y_i_, y_j_\n",
    "\n",
    "    def training_step(self, data, batch_idx):\n",
    "\n",
    "        _, y_i_, y_j_ = self(data['QM9'])\n",
    "        mse1 = self.mse_loss(input=y_i_.flatten(), target=data['QM9']['y_i'].y_norm)\n",
    "        mse2 = self.mse_loss(input=y_j_.flatten(), target=data['QM9']['y_j'].y_norm)\n",
    "        mse = mse1 + mse2\n",
    "\n",
    "        y_ij, _, _ = self(data['DDI'])\n",
    "        y_pred = y_ij.squeeze()\n",
    "        y_true = data['DDI'].binary_y.float()\n",
    "        bce = self.bce_loss(input=y_pred, target=y_true)\n",
    "\n",
    "        loss = mse + bce\n",
    "        \n",
    "        wandb.log({\"train/mse1_loss\": mse1.cpu().detach()})\n",
    "        wandb.log({\"train/mse2_loss\": mse2.cpu().detach()})\n",
    "        wandb.log({\"train/bce_loss\": bce.cpu().detach()})\n",
    "\n",
    "        wandb.log({\"train/loss\": loss.cpu().detach()})\n",
    "\n",
    "        wandb.log({'train/y_i_pred': y_i_.flatten().cpu().detach()})\n",
    "        wandb.log({'train/y_i_true': data['QM9']['y_i'].y_norm.cpu().detach()})\n",
    "        wandb.log({'train/y_j_pred': y_j_.flatten().cpu().detach()})\n",
    "        wandb.log({'train/y_j_true': data['QM9']['y_j'].y_norm.cpu().detach()})\n",
    "\n",
    "        wandb.log({\"train/y_pred\": wandb.Histogram(y_pred.cpu().detach())})\n",
    "        wandb.log({\"train/y_true\": wandb.Histogram(y_true.cpu().detach())})\n",
    "\n",
    "        return {'loss': loss}  # , 'train_accuracy': acc, 'train_f1': f1}\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx, loader_idx):\n",
    "\n",
    "        y_ij, y_i_, y_j_ = self(val_batch)\n",
    "\n",
    "        if loader_idx == 1:\n",
    "            y_pred = y_ij.squeeze()\n",
    "            y_true = val_batch.binary_y.float()\n",
    "            bce = self.bce_loss(input=y_pred, target=y_true)\n",
    "            wandb.log({\"val/loss\": bce})\n",
    "            loss = bce\n",
    "\n",
    "        if loader_idx == 0:\n",
    "            mse1 = self.mse_loss(input=y_i_.flatten(), target=val_batch['y_i'].y_norm)\n",
    "            mse2 = self.mse_loss(input=y_j_.flatten(), target=val_batch['y_j'].y_norm)\n",
    "            mse = mse1 + mse2\n",
    "            wandb.log({\"val/loss\": mse})\n",
    "            loss = mse\n",
    "\n",
    "        # print(val_batch, loader_idx, batch_idx)\n",
    "        # print(ddi_batch, type(ddi_batch))\n",
    "\n",
    "        # y_ij, y_i_, y_j_ = self(val_batch)\n",
    "        # y_pred = y_ij.squeeze()\n",
    "        # y_true = val_batch.binary_y.float()\n",
    "        #\n",
    "        # mse1 = self.mse_loss(input=y_i_.flatten(), target=val_batch['y_i'].y_norm)\n",
    "        # mse2 = self.mse_loss(input=y_j_.flatten(), target=val_batch['y_j'].y_norm)\n",
    "        # mse = mse1 + mse2\n",
    "        # bce = self.bce_loss(input=y_pred, target=y_true)\n",
    "        # loss = mse\n",
    "        # # self.log('validation_loss', bce_loss)\n",
    "        # # self.log('Predicted', y_pred)\n",
    "        # # self.log('Actual', y_true)\n",
    "        # wandb.log({\"val/loss\": loss})\n",
    "\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.lr, betas=(0.28, 0.93), weight_decay=0.01)\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, '25,35', gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        qm9_dataloader = tg.loader.DataLoader(list(self.qm9_dataset), batch_size=self.batch_size,\n",
    "                                              num_workers=self.num_workers, pin_memory=False, shuffle=True)\n",
    "\n",
    "        ddi_dataloader = tg.loader.DataLoader(list(self.ddi_dataset), batch_size=self.batch_size,\n",
    "                                              num_workers=self.num_workers, pin_memory=False, shuffle=True)\n",
    "\n",
    "        loaders = {\"QM9\": qm9_dataloader, 'DDI': ddi_dataloader}\n",
    "        return loaders\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        qm9_dataloader = tg.loader.DataLoader(list(self.qm9_dataset), batch_size=self.batch_size,\n",
    "                                              num_workers=self.num_workers, pin_memory=False, shuffle=True)\n",
    "\n",
    "        ddi_dataloader = tg.loader.DataLoader(list(self.ddi_dataset), batch_size=self.batch_size,\n",
    "                                              num_workers=self.num_workers, pin_memory=False, shuffle=True)\n",
    "        # loaders = {\"QM9\": qm9_dataloader, 'DDI': ddi_dataloader}\n",
    "        loaders = [qm9_dataloader, ddi_dataloader]\n",
    "        return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46a61985-63b9-4100-96c7-7ab07788d1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/syntensor/flux/runs/2b7avhc2\" target=\"_blank\">leafy-pine-371</a></strong> to <a href=\"https://wandb.ai/syntensor/flux\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "data_dir = os.path.join('GraphCoAttention', 'data')\n",
    "trainer = pl.Trainer(gpus=[0], max_epochs=2000, check_val_every_n_epoch=500, accumulate_grad_batches=1)\n",
    "learner = Learner(data_dir, bs=20, lr=0.001, n_cycles=40, hidden_dim=225, n_head=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c1190e3-42c4-4f2e-8c64-fe4b4ae69775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qm9_dataloader = tg.loader.DataLoader(list(learner.qm9_dataset), batch_size=learner.batch_size,\n",
    "                                      num_workers=learner.num_workers, pin_memory=False, shuffle=True)\n",
    "\n",
    "ddi_dataloader = tg.loader.DataLoader(list(learner.ddi_dataset), batch_size=learner.batch_size,\n",
    "                                      num_workers=learner.num_workers, pin_memory=False, shuffle=True)\n",
    "\n",
    "loaders = {\"QM9\": qm9_dataloader, 'DDI': ddi_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "629738c8-9f70-40ba-a7a9-459e7f3bbcf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = next(iter(loaders['DDI']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fb4af101-18e3-4648-9596-cc0e4817b14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(\n",
       "  binary_y=[5],\n",
       "  \u001b[1mx_i\u001b[0m={\n",
       "    x=[122, 9],\n",
       "    batch=[122],\n",
       "    ptr=[6]\n",
       "  },\n",
       "  \u001b[1mx_j\u001b[0m={\n",
       "    x=[100, 9],\n",
       "    batch=[100],\n",
       "    ptr=[6]\n",
       "  },\n",
       "  \u001b[1m(x_i, inner_edge_i, x_i)\u001b[0m={\n",
       "    edge_index=[2, 260],\n",
       "    edge_attr=[260, 3]\n",
       "  },\n",
       "  \u001b[1m(x_j, inner_edge_j, x_j)\u001b[0m={\n",
       "    edge_index=[2, 214],\n",
       "    edge_attr=[130, 3]\n",
       "  },\n",
       "  \u001b[1m(x_i, outer_edge_ij, x_j)\u001b[0m={\n",
       "    edge_index=[2, 2449],\n",
       "    edge_attr=[130, 3]\n",
       "  },\n",
       "  \u001b[1m(x_j, outer_edge_ji, x_i)\u001b[0m={ edge_index=[2, 2449] }\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef6d61f2-abd3-4201-b5b9-d9ea2cf78d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[5., 0., 4.,  ..., 2., 0., 0.],\n",
       "        [7., 0., 2.,  ..., 1., 0., 0.],\n",
       "        [5., 0., 3.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [7., 0., 2.,  ..., 1., 0., 0.],\n",
       "        [7., 0., 2.,  ..., 1., 0., 0.],\n",
       "        [7., 0., 2.,  ..., 2., 0., 0.]]), 'batch': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4]), 'ptr': tensor([  0,  33,  55,  82, 109, 122])}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['x_i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb403ebb-e9bd-4b2e-987c-adf344f67bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aaac47-91c4-4736-8445-727af8797930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "705b7532-27db-4450-b3cf-c83468c9c0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 31904... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d['binary_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96599aed-bcaa-4d71-84a7-fd44a18cb437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e1df5-bb2c-456c-8fb9-b0f0d8654fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654a918-29c4-4be8-b36c-c1a34cf35c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab9b49-2c24-407a-8470-f24be050de19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077401c0-98fb-4d8f-9b99-b5f8be89bc35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f83e4-0f85-4fcb-92f8-0f0260e5a63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad639ca7-9317-4766-873d-4faa04d2b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y_i_, y_j_ = self(learner.qm9_dataset.data)\n",
    "mse1 = self.mse_loss(input=y_i_.flatten(), target=data['QM9']['y_i'].y_norm)\n",
    "mse2 = self.mse_loss(input=y_j_.flatten(), target=data['QM9']['y_j'].y_norm)\n",
    "mse = mse1 + mse2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f91335e-97cb-4a15-a017-a45372820e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58806e48-6066-44d2-a97d-4b804794c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        y_ij, _, _ = self(data['DDI'])\n",
    "        y_pred = y_ij.squeeze()\n",
    "        y_true = data['DDI'].binary_y.float()\n",
    "        bce = self.bce_loss(input=y_pred, target=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb460c6-ac7b-4636-acbd-55b137d312db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce12b83-255b-404b-8032-594cce84a20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d8141-e969-41a4-a55a-632e358fe360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3375164d-dc72-4d06-8359-2b2cf8609ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "data_dir = os.path.join('GraphCoAttention', 'data')\n",
    "    wandb.init()\n",
    "    wandb_logger = WandbLogger(project='flux', log_model='all')\n",
    "    trainer = pl.Trainer(gpus=[0], max_epochs=2000, check_val_every_n_epoch=500, accumulate_grad_batches=1)\n",
    "    trainer.fit(Learner(data_dir, bs=5, lr=0.001, n_cycles=40, hidden_dim=225, n_head=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
